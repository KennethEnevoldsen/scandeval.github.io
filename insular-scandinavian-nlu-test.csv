model_id,num_model_parameters,vocabulary_size,max_sequence_length,speed,rank,mim_gold_ner_rank,scala_is_rank,nqii_rank,fone_rank,scala_fo_rank,is_rank,fo_rank,mim_gold_ner,scala_is,nqii,fone,scala_fo
vesteinn/FoBERT,124.0,50,512,15623,1.33,1.0,1.48116445620381,2.5127082300165977,1.0,1.0,1.66,1.0,85.04,50.78,17.76,91.31,64.39
microsoft/mdeberta-v3-base,278.0,251,512,9237,1.47,1.1248912048242883,1.2400467929600583,1.0,1.0744403050326503,2.5483171198904118,1.12,1.81,81.12,54.11,30.93,88.6,46.81
vesteinn/ScandiBERT-no-faroese,124.0,50,512,15436,1.91,1.0498144355146781,1.1006556077930714,1.7579527534014256,1.0991643800154565,3.948617796452644,1.3,2.52,83.94,58.64,25.35,88.14,27.71
NbAiLab/nb-roberta-base-scandi-1e4,277.0,250,512,15074,2.04,1.1248912048242883,1.48116445620381,4.293183942682977,1.027415080154381,2.5483171198904118,2.3,1.79,81.83,51.09,6.66,90.52,44.99
mideind/IceBERT-xlmr-ic3,277.0,250,512,11004,2.2,1.0,1.0,3.5938555834818944,1.121903980659804,3.948617796452644,1.86,2.54,84.35,59.12,11.18,87.79,22.51
google/rembert,575.0,250,256,3355,2.24,1.2592860960974628,1.48116445620381,1.0,1.121903980659804,5.329856000627667,1.25,3.23,78.05,48.29,29.38,87.35,14.65
sentence-transformers/use-cmlm-multilingual,470.0,501,512,13305,2.27,1.1248912048242883,1.9439481172129687,3.0124854373250938,1.0744403050326503,3.948617796452644,2.03,2.51,80.91,41.91,13.73,88.81,30.92
jonfd/electra-small-nordic,22.0,96,128,5989,2.37,1.2592860960974628,1.0,4.293183942682977,1.1802419991458448,3.948617796452644,2.18,2.56,77.4,60.64,6.51,85.8,30.88
setu4993/LaBSE,470.0,501,512,13386,2.37,1.1248912048242883,1.9439481172129687,3.5938555834818944,1.0744403050326503,3.948617796452644,2.22,2.51,80.45,36.92,11.75,89.16,22.76
vesteinn/XLMR-ENIS,125.0,50,512,10711,2.5,1.0498144355146781,1.48116445620381,1.3301086970567408,1.1445673111531658,6.277475299836382,1.29,3.71,82.2,48.51,27.06,87.09,3.09
mideind/IceBERT-large,406.0,50,512,5677,2.52,1.0,1.0,3.393374062708853,1.1445673111531658,5.329856000627667,1.8,3.24,85.14,59.31,12.84,86.84,9.82
mideind/IceBERT,124.0,50,512,16697,2.52,1.0,1.0,3.393374062708853,1.1445673111531658,5.329856000627667,1.8,3.24,85.32,60.44,13.31,86.5,10.13
pere/roberta-base-exp-32,277.0,250,512,15081,2.52,1.0498144355146781,2.608837229729881,4.052802872947147,1.0,3.948617796452644,2.57,2.47,83.57,23.07,7.81,90.6,22.86
"gpt-3.5-turbo-0613 (few-shot, val)",unknown,100,4095,1344,2.78,1.4498362649556067,3.4630900150132318,1.3301086970567408,1.6062733274882428,5.329856000627667,2.08,3.47,69.59,7.28,28.5,72.48,8.29
mideind/IceBERT-ic3,124.0,50,512,12119,2.79,1.0,1.48116445620381,3.5938555834818944,1.121903980659804,5.964634047049388,2.03,3.54,85.03,45.06,10.82,87.22,6.23
vesteinn/IceBERT,163.0,50,512,12360,2.79,1.0,1.2400467929600583,3.393374062708853,1.1445673111531658,6.277475299836382,1.88,3.71,85.34,55.88,13.31,87.13,3.66
mideind/IceBERT-igc,124.0,50,512,12551,2.85,1.1951775373910856,1.2400467929600583,3.838293872578395,1.2544336623856587,5.964634047049388,2.09,3.61,79.85,54.38,9.91,83.82,4.93
xlm-roberta-large,559.0,250,512,6663,3.0,1.0498144355146781,2.608837229729881,3.0124854373250938,1.0991643800154565,6.474261595569551,2.22,3.79,82.83,22.78,15.72,87.85,1.17
intfloat/multilingual-e5-large,559.0,250,512,6732,3.12,1.1951775373910856,3.4630900150132318,3.0124854373250938,1.0991643800154565,6.277475299836382,2.56,3.69,78.43,10.78,13.79,88.39,2.85
Geotrend/bert-base-25lang-cased,151.0,85,512,13908,3.14,1.3801299439181423,3.8815234212573015,3.838293872578395,1.1802419991458448,5.329856000627667,3.03,3.26,74.65,2.89,9.29,86.09,15.24
mideind/IceBERT-mC4-is,163.0,50,512,12308,3.17,1.1951775373910856,3.000490916056738,5.201596816243907,1.0991643800154565,5.329856000627667,3.13,3.21,79.19,20.95,0.0,88.44,11.83
AI-Sweden-Models/roberta-large-1350k,354.0,50,512,5744,3.25,1.3801299439181423,3.4630900150132318,3.5938555834818944,1.0991643800154565,6.277475299836382,2.81,3.69,73.92,11.77,11.84,88.21,3.16
cardiffnlp/twitter-xlm-roberta-base,277.0,250,512,14837,3.28,1.4498362649556067,2.608837229729881,4.052802872947147,1.2544336623856587,6.474261595569551,2.7,3.86,72.69,28.72,8.46,83.96,1.05
Geotrend/bert-base-da-cased,103.0,23,512,15432,3.31,1.3801299439181423,3.7421577451911046,3.5938555834818944,1.1445673111531658,6.277475299836382,2.91,3.71,73.81,6.23,10.57,86.62,3.64
mistralai/Mistral-7B-v0.1 (few-shot),7242.0,32,32768,2657,3.33,2.261391721212015,3.9677812717682537,1.3301086970567408,1.9877961383415408,6.277475299836382,2.52,4.13,47.24,1.35,26.26,62.63,2.84
bert-base-multilingual-uncased,167.0,106,512,13993,3.34,1.8698190974597684,3.000490916056738,3.838293872578395,1.6062733274882428,5.964634047049388,2.9,3.79,60.88,13.5,9.65,73.06,5.48
AI-Sweden-Models/roberta-large-1160k,354.0,50,512,5741,3.37,1.3801299439181423,3.8815234212573015,3.5938555834818944,1.0991643800154565,6.474261595569551,2.95,3.79,74.3,2.06,11.47,88.24,1.73
NbAiLab/nb-roberta-base-scandinavian,125.0,50,512,14051,3.38,1.5796870338857931,3.8815234212573015,4.148238549301103,1.1802419991458448,5.964634047049388,3.2,3.57,69.04,3.34,7.17,86.1,6.28
KennethEnevoldsen/dfm-sentence-encoder-medium,124.0,50,512,14998,3.4,1.6794026434416454,4.044925098443848,3.393374062708853,1.2170798629074817,6.277475299836382,3.04,3.75,64.88,-0.6,12.39,84.92,2.96
microsoft/xlm-align-base,277.0,250,512,14744,3.4,1.2592860960974628,3.7421577451911046,3.5938555834818944,1.1802419991458448,6.662490159323861,2.87,3.92,78.01,5.92,10.47,85.97,0.02
"mlabonne/NeuralBeagle14-7B (few-shot, val)",7242.0,32,8192,2549,3.4,2.261391721212015,3.9677812717682537,1.7579527534014256,1.9877961383415408,6.277475299836382,2.66,4.13,49.86,1.26,22.42,62.78,3.69
KBLab/megatron-bert-large-swedish-cased-165k,369.0,64,512,7138,3.42,1.7814041688314273,3.7421577451911046,4.148238549301103,1.2960212535776612,5.964634047049388,3.22,3.63,63.35,4.94,7.02,82.76,7.58
KennethEnevoldsen/dfm-sentence-encoder-medium-2,124.0,50,512,14965,3.42,1.6794026434416454,3.9677812717682537,3.5938555834818944,1.2170798629074817,6.277475299836382,3.08,3.75,64.57,0.86,10.76,84.72,3.79
microsoft/infoxlm-base,277.0,250,512,14918,3.42,1.2592860960974628,3.9677812717682537,3.838293872578395,1.1802419991458448,6.474261595569551,3.02,3.83,77.09,1.71,8.56,85.58,0.35
vesteinn/DanskBERT,124.0,50,512,15749,3.43,1.6794026434416454,4.044925098443848,3.5938555834818944,1.2170798629074817,6.277475299836382,3.11,3.75,65.29,-0.03,10.49,85.04,4.48
KennethEnevoldsen/dfm-sentence-encoder-large-1,354.0,50,512,6245,3.44,2.261391721212015,3.8815234212573015,4.052802872947147,1.6062733274882428,5.329856000627667,3.4,3.47,48.31,3.18,7.94,72.99,13.4
KBLab/megatron-bert-large-swedish-cased-110k,369.0,64,512,7075,3.45,1.7814041688314273,3.8815234212573015,4.148238549301103,1.2960212535776612,5.964634047049388,3.27,3.63,63.11,3.47,7.76,82.36,5.2
flax-community/nordic-roberta-wiki,124.0,50,512,16227,3.47,1.7814041688314273,3.8815234212573015,4.293183942682977,1.2960212535776612,5.964634047049388,3.32,3.63,63.31,2.47,5.99,82.64,8.03
sentence-transformers/stsb-xlm-r-multilingual,277.0,250,512,15040,3.48,1.6794026434416454,4.044925098443848,3.838293872578395,1.2544336623856587,6.277475299836382,3.19,3.77,66.23,0.04,10.04,82.97,2.93
Twitter/twhin-bert-large,560.0,250,512,5299,3.49,1.4498362649556067,3.8815234212573015,4.052802872947147,1.2170798629074817,6.474261595569551,3.13,3.85,71.48,2.2,8.19,84.73,1.37
patrickvonplaten/norwegian-roberta-base,124.0,50,512,15698,3.5,1.8698190974597684,3.9677812717682537,4.293183942682977,1.2960212535776612,5.964634047049388,3.38,3.63,60.79,1.29,6.64,82.57,5.74
flax-community/swe-roberta-wiki-oscar,124.0,50,512,15437,3.52,1.7814041688314273,3.9677812717682537,4.399872140932035,1.3714465938380058,5.964634047049388,3.38,3.67,62.23,1.45,5.52,80.52,6.51
AI-Sweden-Models/gpt-sw3-20b (few-shot),20918.0,64,2048,4880,3.54,3.033064354602082,3.7421577451911046,1.3301086970567408,2.4754083166757175,6.277475299836382,2.7,4.38,27.54,4.61,28.12,46.5,3.95
DeepPavlov/rubert-base-cased,177.0,120,512,15785,3.54,1.7814041688314273,3.8815234212573015,4.293183942682977,1.2544336623856587,6.277475299836382,3.32,3.77,61.95,2.4,6.04,83.15,3.21
KB/bert-base-swedish-cased,124.0,50,512,16181,3.59,1.8698190974597684,3.9677812717682537,4.399872140932035,1.2544336623856587,6.277475299836382,3.41,3.77,60.09,1.76,5.57,82.76,3.98
mistralai/Mistral-7B-Instruct-v0.2 (few-shot),7242.0,32,32768,2538,3.6,2.5015137262724343,3.8815234212573015,2.5127082300165977,1.9877961383415408,6.474261595569551,2.97,4.23,43.11,3.4,19.03,61.28,1.68
sentence-transformers/quora-distilbert-multilingual,135.0,120,512,26458,3.6,1.7814041688314273,3.9677812717682537,4.293183942682977,1.2544336623856587,6.474261595569551,3.35,3.86,63.36,1.02,6.48,82.91,1.67
sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2,118.0,250,512,17428,3.62,1.7814041688314273,3.8815234212573015,4.700818385992205,1.2960212535776612,6.277475299836382,3.45,3.79,62.44,1.91,3.69,82.24,2.84
DDSC/roberta-base-danish,125.0,50,512,15004,3.67,1.8698190974597684,3.9677812717682537,4.399872140932035,1.3714465938380058,6.474261595569551,3.41,3.92,59.63,1.76,5.55,80.21,1.1
danish-foundation-models/encoder-large-v1,354.0,50,512,6671,3.67,2.261391721212015,3.9677812717682537,3.838293872578395,1.6526548565232508,6.277475299836382,3.36,3.97,49.68,0.33,9.3,72.46,2.93
dbmdz/bert-medium-historic-multilingual-cased,42.0,32,512,24291,3.68,1.9401407604951777,3.9677812717682537,4.399872140932035,1.3714465938380058,6.474261595569551,3.44,3.92,58.9,0.27,5.9,80.58,1.58
roberta-base,124.0,50,512,13354,3.68,1.8698190974597684,3.9677812717682537,4.293183942682977,1.2960212535776612,6.662490159323861,3.38,3.98,60.18,1.07,6.66,81.78,-1.18
deepset/gbert-base,109.0,31,512,16043,3.69,2.011743937039734,4.044925098443848,4.293183942682977,1.3714465938380058,6.474261595569551,3.45,3.92,56.89,-0.13,6.69,80.48,0.6
pdelobelle/robbert-v2-dutch-base,116.0,40,512,15481,3.71,2.011743937039734,3.9677812717682537,4.399872140932035,1.440572991949908,6.474261595569551,3.46,3.96,55.54,1.06,5.42,78.59,0.65
RuterNorway/Llama-2-13b-chat-norwegian (few-shot),unknown,32,4096,7778,3.72,2.9498559279294496,3.8815234212573015,2.5127082300165977,1.9877961383415408,6.662490159323861,3.11,4.33,28.35,3.14,19.8,60.54,-0.33
mistralai/Mistral-7B-Instruct-v0.1 (few-shot),7242.0,32,32768,5443,3.73,2.752747714486167,4.044925098443848,2.5127082300165977,2.2429156764080713,6.474261595569551,3.1,4.36,36.04,-0.36,17.92,55.42,1.11
meta-llama/Llama-2-7b-chat-hf (few-shot),6738.0,32,4096,2643,3.76,2.5015137262724343,4.044925098443848,3.0124854373250938,1.9877961383415408,6.662490159323861,3.19,4.33,41.1,-1.07,16.12,59.77,-0.54
DDSC/roberta-base-scandinavian,124.0,50,512,14491,3.77,2.011743937039734,3.9677812717682537,4.497356874239228,1.6526548565232508,6.474261595569551,3.49,4.06,51.53,0.89,5.19,63.86,0.73
meta-llama/Llama-2-7b-hf (few-shot),6738.0,32,4096,2648,3.77,2.752747714486167,3.9677812717682537,2.5127082300165977,2.2429156764080713,6.662490159323861,3.08,4.45,32.71,0.66,18.31,52.34,0.11
KBLab/albert-base-swedish-cased-alpha,14.0,50,512,15925,3.79,2.5015137262724343,3.9677812717682537,4.148238549301103,1.6062733274882428,6.474261595569551,3.54,4.04,42.07,0.27,7.35,73.8,0.81
sarnikowski/convbert-medium-small-da-cased,24.0,29,512,13821,3.9,3.033064354602082,3.8815234212573015,4.399872140932035,2.096016587456652,5.964634047049388,3.77,4.03,28.16,2.05,5.37,59.66,4.58
sarnikowski/convbert-small-da-cased,13.0,29,512,14273,3.94,3.1281073682636777,3.9677812717682537,4.399872140932035,2.1361572248683656,5.964634047049388,3.83,4.05,25.49,1.63,5.28,58.5,5.96
Maltehb/aelaectra-danish-electra-small-uncased,14.0,32,128,5995,3.96,2.9498559279294496,3.7421577451911046,5.145149051431321,1.9877961383415408,5.964634047049388,3.95,3.98,30.5,3.59,0.06,62.07,5.11
danish-foundation-models/encoder-small-v1,22.0,96,128,6002,3.98,2.9498559279294496,4.044925098443848,5.145149051431321,1.3714465938380058,6.474261595569551,4.05,3.92,28.99,-0.17,0.42,79.97,0.93
ltg/norbert2,125.0,50,512,15523,3.98,2.9498559279294496,3.8815234212573015,4.700818385992205,1.9877961383415408,6.277475299836382,3.84,4.13,28.74,3.0,3.47,60.57,4.16
Rijgersberg/GEITje-7B (few-shot),7242.0,32,32768,10401,3.99,3.1281073682636777,4.044925098443848,3.393374062708853,2.2429156764080713,6.662490159323861,3.52,4.45,22.55,-0.44,11.98,54.17,0.0
TurkuNLP/bert-base-finnish-cased-v1,124.0,50,512,16701,4.08,2.752747714486167,3.9677812717682537,4.497356874239228,2.3866676519451744,6.474261595569551,3.74,4.43,34.58,0.55,5.07,51.26,1.77
asafaya/bert-base-arabic,110.0,32,512,16347,4.21,3.1281073682636777,4.044925098443848,4.497356874239228,2.3866676519451744,6.662490159323861,3.89,4.52,22.51,0.23,5.05,50.44,-0.06
Maltehb/danish-bert-botxo,110.0,32,512,16091,4.24,3.5849109256851017,4.044925098443848,4.497356874239228,2.598116812904076,6.277475299836382,4.04,4.44,12.64,0.06,4.77,43.59,3.13
fresh-xlm-roberta-base,277.0,250,512,1319,4.28,3.4177335364522636,4.044925098443848,5.06308231076627,2.4754083166757175,6.277475299836382,4.18,4.38,17.34,-0.06,1.02,48.7,2.37
alexanderfalk/danbert-small-cased,83.0,52,512,30013,4.36,3.5849109256851017,3.9677812717682537,4.970448532141841,2.598116812904076,6.474261595569551,4.17,4.54,12.39,1.63,1.7,45.16,2.24
01-ai/Yi-6B (few-shot),6061.0,64,4096,2786,4.44,4.022884940997072,3.8815234212573015,2.5127082300165977,4.161967302921374,6.662490159323861,3.47,5.41,0.0,2.12,16.85,0.0,-0.28
Qwen/Qwen1.5-4B-Chat (few-shot),3950.0,152,32768,4347,4.55,4.022884940997072,4.044925098443848,3.0124854373250938,4.157317323704167,6.662490159323861,3.69,5.41,0.05,-0.35,14.46,0.13,-0.42
Qwen/Qwen1.5-4B (few-shot),3950.0,152,32768,3248,4.55,4.022884940997072,4.044925098443848,3.0124854373250938,4.157317323704167,6.662490159323861,3.69,5.41,0.0,-0.55,14.11,0.06,-0.36
Qwen/Qwen1.5-1.8B-Chat (few-shot),1837.0,152,32768,8304,4.66,4.022884940997072,3.9677812717682537,4.052802872947147,4.1487731999378905,6.474261595569551,4.01,5.31,0.25,0.78,7.92,0.38,0.15
RuterNorway/Llama-2-7b-chat-norwegian (few-shot),unknown,32,4096,10890,4.66,3.680111456598537,4.044925098443848,4.970448532141841,3.508947073429889,6.662490159323861,4.23,5.09,9.48,0.07,1.04,18.86,-0.43
fresh-electra-small,13.0,31,512,7219,4.7,3.680111456598537,4.044925098443848,5.145149051431321,3.7429879509347184,6.474261595569551,4.29,5.11,9.96,-0.1,0.12,12.1,0.64
Qwen/Qwen1.5-1.8B (few-shot),1837.0,152,32768,5666,4.71,4.022884940997072,3.9677812717682537,4.293183942682977,4.157317323704167,6.474261595569551,4.09,5.32,0.03,0.94,6.31,0.09,1.14
Qwen/Qwen1.5-0.5B-Chat (few-shot),620.0,152,32768,11740,4.81,4.022884940997072,3.9677812717682537,4.700818385992205,4.117020254428714,6.662490159323861,4.23,5.39,0.32,1.76,3.1,1.3,-0.54
Qwen/Qwen1.5-0.5B (few-shot),620.0,152,32768,11371,4.82,3.9938144164433007,4.044925098443848,4.700818385992205,4.117020254428714,6.662490159323861,4.25,5.39,1.14,-0.57,3.31,1.18,-1.47
ltg/norbert,112.0,33,512,16280,4.88,4.022884940997072,4.044925098443848,4.970448532141841,4.161967302921374,6.662490159323861,4.35,5.41,0.0,-0.03,1.68,0.0,-1.21
RJuro/kanelsnegl-v0.1 (few-shot),7242.0,32,512,9757,4.92,4.022884940997072,4.044925098443848,5.201596816243907,4.161967302921374,6.662490159323861,4.42,5.41,0.0,0.0,0.0,0.0,0.0
ai-forever/mGPT (few-shot),unknown,100,1024,13551,4.92,4.022884940997072,4.044925098443848,5.201596816243907,4.161967302921374,6.662490159323861,4.42,5.41,0.0,0.0,0.0,0.0,-0.24
