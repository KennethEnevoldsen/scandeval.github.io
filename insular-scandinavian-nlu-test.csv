model_id,num_model_parameters,vocabulary_size,max_sequence_length,speed,score,mim_gold_ner_score,scala_is_score,nqii_score,fone_score,scala_fo_score,is_score,fo_score,mim_gold_ner,scala_is,nqii,fone,scala_fo
vesteinn/FoBERT,124.0,50,512,15623,4.67,0.0,0.4777105133878402,1.500269628044602,0.0,0.0,4.34,5.0,85.04,50.78,17.76,91.31,64.39
microsoft/mdeberta-v3-base,278.0,251,512,9237,4.54,0.12875542781347868,0.2383236650661521,0.0,0.07875307270577267,1.5367639396145572,4.88,4.19,81.12,54.11,30.93,88.6,46.81
vesteinn/ScandiBERT-no-faroese,124.0,50,512,15436,4.09,0.05135572969292057,0.09993307164364996,0.7517203072323221,0.10490955975737573,2.9266158999875183,4.7,3.48,83.94,58.64,25.35,88.14,27.71
NbAiLab/nb-roberta-base-scandi-1e4,277.0,250,512,15074,3.96,0.12875542781347868,0.4777105133878402,3.2661049571451306,0.029003398087710575,1.5367639396145572,3.71,4.22,81.83,51.09,6.66,90.52,44.99
mideind/IceBERT-xlmr-ic3,277.0,250,512,11004,3.8,0.0,0.0,2.5725269911363524,0.12896660012091338,2.9266158999875183,4.14,3.47,84.35,59.12,11.18,87.79,22.51
google/rembert,575.0,250,256,3355,3.77,0.26730859291560827,0.4777105133878402,0.0,0.12896660012091338,4.297547627684479,4.75,2.79,78.05,48.29,29.38,87.35,14.65
sentence-transformers/use-cmlm-multilingual,470.0,501,512,13305,3.74,0.12875542781347868,0.9371721744431751,1.9959372988059756,0.07875307270577267,2.9266158999875183,3.98,3.5,80.91,41.91,13.73,88.81,30.92
setu4993/LaBSE,470.0,501,512,13386,3.65,0.12875542781347868,0.9371721744431751,2.5725269911363524,0.07875307270577267,2.9266158999875183,3.79,3.5,80.45,36.92,11.75,89.16,22.76
jonfd/electra-small-nordic,22.0,96,128,5989,3.63,0.26730859291560827,0.0,3.2661049571451306,0.19068448546980862,2.9266158999875183,3.82,3.44,77.4,60.64,6.51,85.8,30.88
vesteinn/XLMR-ENIS,125.0,50,512,10711,3.5,0.05135572969292057,0.4777105133878402,0.3273943000509561,0.15294295155198093,5.238096013282544,4.71,2.3,82.2,48.51,27.06,87.09,3.09
mideind/IceBERT-large,406.0,50,512,5677,3.49,0.0,0.0,2.3736939771871346,0.15294295155198093,4.297547627684479,4.21,2.77,85.14,59.31,12.84,86.84,9.82
mideind/IceBERT,124.0,50,512,16697,3.49,0.0,0.0,2.3736939771871346,0.15294295155198093,4.297547627684479,4.21,2.77,85.32,60.44,13.31,86.5,10.13
pere/roberta-base-exp-32,277.0,250,512,15081,3.49,0.05135572969292057,1.5972885134437051,3.0277004777316883,0.0,2.9266158999875183,3.44,3.54,83.57,23.07,7.81,90.6,22.86
mideind/IceBERT-ic3,124.0,50,512,12119,3.23,0.0,0.4777105133878402,2.5725269911363524,0.12896660012091338,4.927589108766159,3.98,2.47,85.03,45.06,10.82,87.22,6.23
"gpt-3.5-turbo-0613 (few-shot, val)",unknown,100,4095,1344,3.22,0.4637545199588997,2.4454092159584535,0.3273943000509561,0.6413983314322861,4.297547627684479,3.92,2.53,69.59,7.28,28.5,72.48,8.29
vesteinn/IceBERT,163.0,50,512,12360,3.21,0.0,0.2383236650661521,2.3736939771871346,0.15294295155198093,5.238096013282544,4.13,2.3,85.34,55.88,13.31,87.13,3.66
mideind/IceBERT-igc,124.0,50,512,12551,3.16,0.2012164696603458,0.2383236650661521,2.8149553284626077,0.269174511091339,4.927589108766159,3.92,2.4,79.85,54.38,9.91,83.82,4.93
xlm-roberta-large,559.0,250,512,6663,3.01,0.05135572969292057,1.5972885134437051,1.9959372988059756,0.10490955975737573,5.433413935695276,3.79,2.23,82.83,22.78,15.72,87.85,1.17
intfloat/multilingual-e5-large,559.0,250,512,6732,2.89,0.2012164696603458,2.4454092159584535,1.9959372988059756,0.10490955975737573,5.238096013282544,3.45,2.33,78.43,10.78,13.79,88.39,2.85
Geotrend/bert-base-25lang-cased,151.0,85,512,13908,2.87,0.3918914356119306,2.86083898168248,2.8149553284626077,0.19068448546980862,4.297547627684479,2.98,2.76,74.65,2.89,9.29,86.09,15.24
mideind/IceBERT-mC4-is,163.0,50,512,12308,2.84,0.2012164696603458,1.9861307921140008,4.167048190536036,0.10490955975737573,4.297547627684479,2.88,2.8,79.19,20.95,0.0,88.44,11.83
AI-Sweden-Models/roberta-large-1350k,354.0,50,512,5744,2.77,0.3918914356119306,2.4454092159584535,2.5725269911363524,0.10490955975737573,5.238096013282544,3.2,2.33,73.92,11.77,11.84,88.21,3.16
cardiffnlp/twitter-xlm-roberta-base,277.0,250,512,14837,2.72,0.4637545199588997,1.5972885134437051,3.0277004777316883,0.269174511091339,5.433413935695276,3.3,2.15,72.69,28.72,8.46,83.96,1.05
Geotrend/bert-base-da-cased,103.0,23,512,15432,2.7,0.3918914356119306,2.7224737142487894,2.5725269911363524,0.15294295155198093,5.238096013282544,3.1,2.3,73.81,6.23,10.57,86.62,3.64
mistralai/Mistral-7B-v0.1 (few-shot),7242.0,32,32768,2657,2.67,1.3004200810900346,2.946477647465109,0.3273943000509561,1.045025017927754,5.238096013282544,3.48,1.86,47.24,1.35,26.26,62.63,2.84
bert-base-multilingual-uncased,167.0,106,512,13993,2.66,0.8967319209653922,1.9861307921140008,2.8149553284626077,0.6413983314322861,4.927589108766159,3.1,2.22,60.88,13.5,9.65,73.06,5.48
AI-Sweden-Models/roberta-large-1160k,354.0,50,512,5741,2.65,0.3918914356119306,2.86083898168248,2.5725269911363524,0.10490955975737573,5.433413935695276,3.06,2.23,74.3,2.06,11.47,88.24,1.73
KennethEnevoldsen/dfm-sentence-encoder-medium,124.0,50,512,14998,2.62,0.7004238459946797,3.023067712609677,2.3736939771871346,0.22965658481670265,5.238096013282544,2.97,2.27,64.88,-0.6,12.39,84.92,2.96
NbAiLab/nb-roberta-base-scandinavian,125.0,50,512,14051,2.62,0.5976229643304434,2.86083898168248,3.1223514116161186,0.19068448546980862,4.927589108766159,2.81,2.44,69.04,3.34,7.17,86.1,6.28
microsoft/xlm-align-base,277.0,250,512,14744,2.62,0.26730859291560827,2.7224737142487894,2.5725269911363524,0.19068448546980862,5.620237981923955,3.15,2.09,78.01,5.92,10.47,85.97,0.02
KennethEnevoldsen/dfm-sentence-encoder-medium-2,124.0,50,512,14965,2.6,0.7004238459946797,2.946477647465109,2.5725269911363524,0.22965658481670265,5.238096013282544,2.93,2.27,64.57,0.86,10.76,84.72,3.79
"mlabonne/NeuralBeagle14-7B (few-shot, val)",7242.0,32,8192,2549,2.6,1.3004200810900346,2.946477647465109,0.7517203072323221,1.045025017927754,5.238096013282544,3.33,1.86,49.86,1.26,22.42,62.78,3.69
microsoft/infoxlm-base,277.0,250,512,14918,2.59,0.26730859291560827,2.946477647465109,2.8149553284626077,0.19068448546980862,5.433413935695276,2.99,2.19,77.09,1.71,8.56,85.58,0.35
KBLab/megatron-bert-large-swedish-cased-165k,369.0,64,512,7138,2.58,0.8055813713597858,2.7224737142487894,3.1223514116161186,0.31317151770442586,4.927589108766159,2.78,2.38,63.35,4.94,7.02,82.76,7.58
vesteinn/DanskBERT,124.0,50,512,15749,2.58,0.7004238459946797,3.023067712609677,2.5725269911363524,0.22965658481670265,5.238096013282544,2.9,2.27,65.29,-0.03,10.49,85.04,4.48
KBLab/megatron-bert-large-swedish-cased-110k,369.0,64,512,7075,2.56,0.8055813713597858,2.86083898168248,3.1223514116161186,0.31317151770442586,4.927589108766159,2.74,2.38,63.11,3.47,7.76,82.36,5.2
KennethEnevoldsen/dfm-sentence-encoder-large-1,354.0,50,512,6245,2.56,1.3004200810900346,2.86083898168248,3.0277004777316883,0.6413983314322861,4.297547627684479,2.6,2.53,48.31,3.18,7.94,72.99,13.4
flax-community/nordic-roberta-wiki,124.0,50,512,16227,2.54,0.8055813713597858,2.86083898168248,3.2661049571451306,0.31317151770442586,4.927589108766159,2.69,2.38,63.31,2.47,5.99,82.64,8.03
sentence-transformers/stsb-xlm-r-multilingual,277.0,250,512,15040,2.54,0.7004238459946797,3.023067712609677,2.8149553284626077,0.269174511091339,5.238096013282544,2.82,2.25,66.23,0.04,10.04,82.97,2.93
Twitter/twhin-bert-large,560.0,250,512,5299,2.52,0.4637545199588997,2.86083898168248,3.0277004777316883,0.22965658481670265,5.433413935695276,2.88,2.17,71.48,2.2,8.19,84.73,1.37
patrickvonplaten/norwegian-roberta-base,124.0,50,512,15698,2.5,0.8967319209653922,2.946477647465109,3.2661049571451306,0.31317151770442586,4.927589108766159,2.63,2.38,60.79,1.29,6.64,82.57,5.74
flax-community/swe-roberta-wiki-oscar,124.0,50,512,15437,2.48,0.8055813713597858,2.946477647465109,3.3719158863962444,0.3929666945615764,4.927589108766159,2.63,2.34,62.23,1.45,5.52,80.52,6.51
DeepPavlov/rubert-base-cased,177.0,120,512,15785,2.47,0.8055813713597858,2.86083898168248,3.2661049571451306,0.269174511091339,5.238096013282544,2.69,2.25,61.95,2.4,6.04,83.15,3.21
AI-Sweden-Models/gpt-sw3-20b (few-shot),20918.0,64,2048,4880,2.44,2.095968816358294,2.7224737142487894,0.3273943000509561,1.5608874571766065,5.238096013282544,3.28,1.6,27.54,4.61,28.12,46.5,3.95
KB/bert-base-swedish-cased,124.0,50,512,16181,2.42,0.8967319209653922,2.946477647465109,3.3719158863962444,0.269174511091339,5.238096013282544,2.59,2.25,60.09,1.76,5.57,82.76,3.98
sentence-transformers/quora-distilbert-multilingual,135.0,120,512,26458,2.41,0.8055813713597858,2.946477647465109,3.2661049571451306,0.269174511091339,5.433413935695276,2.66,2.15,63.36,1.02,6.48,82.91,1.67
mistralai/Mistral-7B-Instruct-v0.2 (few-shot),7242.0,32,32768,2538,2.4,1.5479716323180193,2.86083898168248,1.500269628044602,1.045025017927754,5.433413935695276,3.03,1.76,43.11,3.4,19.03,61.28,1.68
sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2,118.0,250,512,17428,2.38,0.8055813713597858,2.86083898168248,3.670387529624422,0.31317151770442586,5.238096013282544,2.55,2.22,62.44,1.91,3.69,82.24,2.84
DDSC/roberta-base-danish,125.0,50,512,15004,2.34,0.8967319209653922,2.946477647465109,3.3719158863962444,0.3929666945615764,5.433413935695276,2.59,2.09,59.63,1.76,5.55,80.21,1.1
danish-foundation-models/encoder-large-v1,354.0,50,512,6671,2.34,1.3004200810900346,2.946477647465109,2.8149553284626077,0.6904670170952047,5.238096013282544,2.65,2.04,49.68,0.33,9.3,72.46,2.93
dbmdz/bert-medium-historic-multilingual-cased,42.0,32,512,24291,2.33,0.969229386430779,2.946477647465109,3.3719158863962444,0.3929666945615764,5.433413935695276,2.57,2.09,58.9,0.27,5.9,80.58,1.58
deepset/gbert-base,109.0,31,512,16043,2.33,1.0430480163476668,3.023067712609677,3.2661049571451306,0.3929666945615764,5.433413935695276,2.56,2.09,56.89,-0.13,6.69,80.48,0.6
roberta-base,124.0,50,512,13354,2.33,0.8967319209653922,2.946477647465109,3.2661049571451306,0.31317151770442586,5.620237981923955,2.63,2.03,60.18,1.07,6.66,81.78,-1.18
pdelobelle/robbert-v2-dutch-base,116.0,40,512,15481,2.3,1.0430480163476668,2.946477647465109,3.3719158863962444,0.46609799425207427,5.433413935695276,2.55,2.05,55.54,1.06,5.42,78.59,0.65
RuterNorway/Llama-2-13b-chat-norwegian (few-shot),unknown,32,4096,7778,2.27,2.010185861593831,2.86083898168248,1.500269628044602,1.045025017927754,5.620237981923955,2.88,1.67,28.35,3.14,19.8,60.54,-0.33
mistralai/Mistral-7B-Instruct-v0.1 (few-shot),7242.0,32,32768,5443,2.26,1.8069789793866637,3.023067712609677,1.500269628044602,1.3149251415396104,5.433413935695276,2.89,1.63,36.04,-0.36,17.92,55.42,1.11
meta-llama/Llama-2-7b-chat-hf (few-shot),6738.0,32,4096,2643,2.24,1.5479716323180193,3.023067712609677,1.9959372988059756,1.045025017927754,5.620237981923955,2.81,1.67,41.1,-1.07,16.12,59.77,-0.54
meta-llama/Llama-2-7b-hf (few-shot),6738.0,32,4096,2648,2.23,1.8069789793866637,2.946477647465109,1.500269628044602,1.3149251415396104,5.620237981923955,2.92,1.53,32.71,0.66,18.31,52.34,0.11
DDSC/roberta-base-scandinavian,124.0,50,512,14491,2.22,1.0430480163476668,2.946477647465109,3.4685990283774344,0.6904670170952047,5.433413935695276,2.51,1.94,51.53,0.89,5.19,63.86,0.73
KBLab/albert-base-swedish-cased-alpha,14.0,50,512,15925,2.21,1.5479716323180193,2.946477647465109,3.1223514116161186,0.6413983314322861,5.433413935695276,2.46,1.96,42.07,0.27,7.35,73.8,0.81
sarnikowski/convbert-medium-small-da-cased,24.0,29,512,13821,2.09,2.095968816358294,2.86083898168248,3.3719158863962444,1.1595153184937654,4.927589108766159,2.22,1.96,28.16,2.05,5.37,59.66,4.58
sarnikowski/convbert-small-da-cased,13.0,29,512,14273,2.05,2.1939525286773316,2.946477647465109,3.3719158863962444,1.2019815407258507,4.927589108766159,2.16,1.94,25.49,1.63,5.28,58.5,5.96
Maltehb/aelaectra-danish-electra-small-uncased,14.0,32,128,5995,2.03,2.010185861593831,2.7224737142487894,4.111064580849192,1.045025017927754,4.927589108766159,2.05,2.01,30.5,3.59,0.06,62.07,5.11
danish-foundation-models/encoder-small-v1,22.0,96,128,6002,2.02,2.010185861593831,3.023067712609677,4.111064580849192,0.3929666945615764,5.433413935695276,1.95,2.09,28.99,-0.17,0.42,79.97,0.93
Rijgersberg/GEITje-7B (few-shot),7242.0,32,32768,10401,2.0,2.1939525286773316,3.023067712609677,2.3736939771871346,1.3149251415396104,5.620237981923955,2.47,1.53,22.55,-0.44,11.98,54.17,0.0
ltg/norbert2,125.0,50,512,15523,2.0,2.010185861593831,2.86083898168248,3.670387529624422,1.045025017927754,5.238096013282544,2.15,1.86,28.74,3.0,3.47,60.57,4.16
TurkuNLP/bert-base-finnish-cased-v1,124.0,50,512,16701,1.9,1.8069789793866637,2.946477647465109,3.4685990283774344,1.467005520255233,5.433413935695276,2.26,1.55,34.58,0.55,5.07,51.26,1.77
asafaya/bert-base-arabic,110.0,32,512,16347,1.78,2.1939525286773316,3.023067712609677,3.4685990283774344,1.467005520255233,5.620237981923955,2.1,1.46,22.51,0.23,5.05,50.44,-0.06
Maltehb/danish-bert-botxo,110.0,32,512,16091,1.75,2.664889914102219,3.023067712609677,3.4685990283774344,1.6907051832169466,5.238096013282544,1.95,1.54,12.64,0.06,4.77,43.59,3.13
fresh-xlm-roberta-base,277.0,250,512,1319,1.71,2.4925399371627024,3.023067712609677,4.029672653411183,1.5608874571766065,5.238096013282544,1.82,1.6,17.34,-0.06,1.02,48.7,2.37
alexanderfalk/danbert-small-cased,83.0,52,512,30013,1.63,2.664889914102219,2.946477647465109,3.9378005779880763,1.6907051832169466,5.433413935695276,1.82,1.44,12.39,1.63,1.7,45.16,2.24
01-ai/Yi-6B (few-shot),6061.0,64,4096,2786,1.51,3.1164151579495973,2.86083898168248,1.500269628044602,3.3451587925522674,5.620237981923955,2.51,0.52,0.0,2.12,16.85,0.0,-0.28
RuterNorway/Llama-2-7b-chat-norwegian (few-shot),unknown,32,4096,10890,1.31,2.763036017369259,3.023067712609677,3.9378005779880763,2.6543052342692013,5.620237981923955,1.76,0.86,9.48,0.07,1.04,18.86,-0.43
Qwen/Qwen1.5-1.8B-Chat (few-shot),1837.0,152,32768,8304,1.29,3.1164151579495973,2.946477647465109,3.0277004777316883,3.3312002770533056,5.433413935695276,1.97,0.62,0.25,0.78,7.92,0.38,0.15
fresh-electra-small,13.0,31,512,7219,1.26,2.763036017369259,3.023067712609677,4.111064580849192,2.901905485694507,5.433413935695276,1.7,0.83,9.96,-0.1,0.12,12.1,0.64
Qwen/Qwen1.5-1.8B (few-shot),1837.0,152,32768,5666,1.25,3.1164151579495973,2.946477647465109,3.2661049571451306,3.341744265702791,5.433413935695276,1.89,0.61,0.03,0.94,6.31,0.09,1.14
Qwen/Qwen1.5-0.5B-Chat (few-shot),620.0,152,32768,11740,1.15,3.1164151579495973,2.946477647465109,3.670387529624422,3.2976076953838747,5.620237981923955,1.76,0.54,0.32,1.76,3.1,1.3,-0.54
Qwen/Qwen1.5-0.5B (few-shot),620.0,152,32768,11371,1.14,3.0864451706238363,3.023067712609677,3.670387529624422,3.2976076953838747,5.620237981923955,1.74,0.54,1.14,-0.57,3.31,1.18,-1.47
ltg/norbert,112.0,33,512,16280,1.08,3.1164151579495973,3.023067712609677,3.9378005779880763,3.3451587925522674,5.620237981923955,1.64,0.52,0.0,-0.03,1.68,0.0,-1.21
RJuro/kanelsnegl-v0.1 (few-shot),7242.0,32,512,9757,1.04,3.1164151579495973,3.023067712609677,4.167048190536036,3.3451587925522674,5.620237981923955,1.56,0.52,0.0,0.0,0.0,0.0,0.0
ai-forever/mGPT (few-shot),unknown,100,1024,13551,1.04,3.1164151579495973,3.023067712609677,4.167048190536036,3.3451587925522674,5.620237981923955,1.56,0.52,0.0,0.0,0.0,0.0,-0.24
