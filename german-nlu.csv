rank,model_id,num_model_parameters,vocabulary_size,max_sequence_length,speed,score,germeval_score,sb10k_score,scala_de_score,germanquad_score,de_score,germeval,sb10k,scala_de,germanquad
1,deepset/gbert-large,335.0,31,512,5463,100.0,100.0,100.0,100.0,100.0,100.0,82.15,66.16,77.48,32.63
2,google/rembert,575.0,250,256,3355,93.31,89.47,90.91,92.86,100.0,93.31,79.32,59.69,72.25,34.48
3,xlm-roberta-large,559.0,250,512,6663,88.78,94.74,81.82,85.71,92.86,88.78,80.27,58.44,62.12,32.23
4,microsoft/mdeberta-v3-base,278.0,251,512,9237,88.31,100.0,81.82,92.86,78.57,88.31,81.05,59.5,71.84,26.25
5,sentence-transformers/use-cmlm-multilingual,470.0,501,512,13305,85.68,89.47,81.82,78.57,92.86,85.68,78.99,59.17,61.58,30.42
6,deepset/gbert-base,109.0,31,512,16043,81.17,100.0,81.82,85.71,57.14,81.17,81.33,57.77,66.13,16.68
7,setu4993/LaBSE,470.0,501,512,13386,78.54,89.47,81.82,71.43,71.43,78.54,78.94,58.2,53.53,23.55
8,dbmdz/bert-base-german-uncased,109.0,31,512,11438,74.48,89.47,72.73,85.71,50.0,74.48,79.93,56.26,68.22,13.87
9,ZurichNLP/unsup-simcse-xlm-roberta-base,277.0,250,512,10471,73.65,84.21,81.82,71.43,57.14,73.65,76.7,57.27,52.46,18.24
10,dbmdz/bert-base-german-cased,109.0,31,512,11844,73.52,94.74,63.64,85.71,50.0,73.52,80.9,53.44,67.08,14.49
11,facebook/xlm-v-base,778.0,902,512,13135,71.86,84.21,81.82,57.14,64.29,71.86,73.96,57.43,35.0,20.81
12,cardiffnlp/twitter-xlm-roberta-base,277.0,250,512,14837,71.05,84.21,100.0,64.29,35.71,71.05,75.55,63.32,49.39,1.43
13,"mlabonne/NeuralBeagle14-7B (few-shot, val)",7242.0,32,8192,2549,68.88,63.16,90.91,42.86,78.57,68.88,64.81,59.6,27.06,25.22
14,"gpt-3.5-turbo-0613 (few-shot, val)",unknown,100,4095,1344,68.37,57.89,72.73,57.14,85.71,68.37,61.5,55.5,38.96,30.2
15,microsoft/infoxlm-base,277.0,250,512,14918,66.04,89.47,81.82,28.57,64.29,66.04,79.97,58.3,11.85,19.63
16,clips/mfaq,277.0,250,128,5591,64.72,84.21,81.82,57.14,35.71,64.72,76.68,59.51,32.54,1.53
17,microsoft/xlm-align-base,277.0,250,512,14744,64.25,89.47,81.82,28.57,57.14,64.25,79.38,58.58,15.34,16.58
18,Twitter/twhin-bert-large,560.0,250,512,5299,64.23,84.21,72.73,50.0,50.0,64.23,74.8,55.04,29.15,11.93
19,jhu-clsp/bernice,277.0,250,128,5567,60.32,78.95,90.91,64.29,7.14,60.32,72.25,62.0,48.1,0.0
20,mistralai/Mistral-7B-v0.1 (few-shot),7242.0,32,32768,2657,59.91,52.63,72.73,42.86,71.43,59.91,55.37,54.27,23.12,22.94
21,sentence-transformers/paraphrase-xlm-r-multilingual-v1,277.0,250,512,14994,58.05,78.95,81.82,57.14,14.29,58.05,71.8,57.71,38.75,0.3
22,sentence-transformers/stsb-xlm-r-multilingual,277.0,250,512,15040,57.54,73.68,63.64,57.14,35.71,57.54,69.4,52.68,34.44,0.73
23,RuterNorway/Llama-2-13b-chat-norwegian (few-shot),unknown,32,4096,7778,57.15,52.63,54.55,35.71,85.71,57.15,56.71,49.77,19.92,27.87
24,sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2,118.0,250,512,17428,55.4,63.16,72.73,50.0,35.71,55.4,65.62,55.6,32.22,0.64
25,mistralai/Mistral-7B-Instruct-v0.2 (few-shot),7242.0,32,32768,2538,55.37,52.63,54.55,42.86,71.43,55.37,55.15,47.85,24.29,23.98
26,dbmdz/bert-base-historic-multilingual-cased,110.0,32,512,15165,52.98,68.42,36.36,64.29,42.86,52.98,67.24,41.36,48.21,7.75
27,mistralai/Mistral-7B-Instruct-v0.1 (few-shot),7242.0,32,32768,5443,52.73,42.11,54.55,35.71,78.57,52.73,51.79,47.27,22.15,24.3
28,01-ai/Yi-6B (few-shot),6061.0,64,4096,2786,50.59,31.58,63.64,21.43,85.71,50.59,44.97,53.14,7.64,30.12
29,meta-llama/Llama-2-7b-chat-hf (few-shot),6738.0,32,4096,2643,47.36,36.84,45.45,28.57,78.57,47.36,50.0,46.54,15.3,25.57
30,Rijgersberg/GEITje-7B (few-shot),7242.0,32,32768,10401,45.22,26.32,54.55,21.43,78.57,45.22,39.09,47.83,10.31,26.13
31,sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking,135.0,120,512,26151,45.03,68.42,54.55,35.71,21.43,45.03,66.75,49.61,17.6,0.36
32,sentence-transformers/distiluse-base-multilingual-cased-v1,135.0,120,512,26344,43.82,47.37,63.64,35.71,28.57,43.82,53.77,53.53,20.15,0.58
33,meta-llama/Llama-2-7b-hf (few-shot),6738.0,32,4096,2648,43.43,26.32,54.55,28.57,64.29,43.43,41.88,50.17,15.82,18.35
34,sentence-transformers/distiluse-base-multilingual-cased-v2,135.0,120,512,17807,40.25,47.37,63.64,35.71,14.29,40.25,53.13,51.94,17.74,0.06
35,AI-Sweden-Models/gpt-sw3-20b (few-shot),20918.0,64,2048,4880,29.94,21.05,27.27,14.29,57.14,29.94,35.78,34.13,2.18,17.99
36,RuterNorway/Llama-2-7b-chat-norwegian (few-shot),unknown,32,4096,10890,28.62,15.79,27.27,7.14,64.29,28.62,27.22,33.54,0.45,20.4
37,3ebdola/Dialectal-Arabic-XLM-R-Base,277.0,250,512,15177,22.8,21.05,27.27,14.29,28.57,22.8,34.18,34.81,1.25,0.46
38,fresh-electra-small,13.0,31,512,7219,10.75,10.53,18.18,7.14,7.14,10.75,9.42,9.76,-0.18,0.0
39,RJuro/kanelsnegl-v0.1 (few-shot),7242.0,32,512,9757,7.16,5.26,9.09,7.14,7.14,7.16,0.0,0.0,0.0,0.0
39,ai-forever/mGPT (few-shot),unknown,100,1024,13551,7.16,5.26,9.09,7.14,7.14,7.16,0.3,0.29,-0.11,0.0
