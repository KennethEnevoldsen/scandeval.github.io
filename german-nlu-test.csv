model_id,num_model_parameters,vocabulary_size,max_sequence_length,speed,score,germeval_score,sb10k_score,scala_de_score,germanquad_score,de_score,germeval,sb10k,scala_de,germanquad
deepset/gbert-large,335.0,31,512,5463,4.96,0.0,0.0,0.0,0.16773028122228284,4.96,82.15,66.16,77.48,32.63
google/rembert,575.0,250,256,3355,4.83,0.08263223993631871,0.3941036226306289,0.2174516584735489,0.0,4.83,79.32,59.69,72.25,34.48
xlm-roberta-large,559.0,250,512,6663,4.75,0.04742366675940341,0.3941036226306289,0.38509352565153865,0.16773028122228284,4.75,80.27,58.44,62.12,32.23
microsoft/mdeberta-v3-base,278.0,251,512,9237,4.7,0.0,0.3941036226306289,0.2174516584735489,0.6003637728684872,4.7,81.05,59.5,71.84,26.25
sentence-transformers/use-cmlm-multilingual,470.0,501,512,13305,4.61,0.12001859723934,0.3941036226306289,0.6609167706507184,0.36851342415839494,4.61,78.99,59.17,61.58,30.42
deepset/gbert-base,109.0,31,512,16043,4.4,0.0,0.3941036226306289,0.38509352565153865,1.6161168637770504,4.4,81.33,57.77,66.13,16.68
setu4993/LaBSE,470.0,501,512,13386,4.38,0.12001859723934,0.3941036226306289,0.9950662372574923,0.9535800459975121,4.38,78.94,58.2,53.53,23.55
dbmdz/bert-base-german-cased,109.0,31,512,11844,4.3,0.04742366675940341,0.7691364013388334,0.38509352565153865,1.6161168637770504,4.3,80.9,53.44,67.08,14.49
dbmdz/bert-base-german-uncased,109.0,31,512,11438,4.26,0.08263223993631871,0.6030758914348258,0.38509352565153865,1.870786227352088,4.26,79.93,56.26,68.22,13.87
ZurichNLP/unsup-simcse-xlm-roberta-base,277.0,250,512,10471,4.25,0.20686896516694914,0.3941036226306289,0.9950662372574923,1.4206163223898995,4.25,76.7,57.27,52.46,18.24
"gpt-3.5-turbo-0613 (few-shot, val)",unknown,100,4095,1344,4.16,0.7839812123763299,0.6030758914348258,1.6006235902411823,0.36851342415839494,4.16,61.5,55.5,38.96,30.2
facebook/xlm-v-base,778.0,902,512,13135,4.12,0.2787877062055595,0.3941036226306289,1.6006235902411823,1.2407841463928768,4.12,73.96,57.43,35.0,20.81
"mlabonne/NeuralBeagle14-7B (few-shot, val)",7242.0,32,8192,2549,4.07,0.6277204589858295,0.3941036226306289,2.094998859914024,0.6003637728684872,4.07,64.81,59.6,27.06,25.22
microsoft/infoxlm-base,277.0,250,512,14918,3.93,0.08263223993631871,0.3941036226306289,2.5619911431080125,1.2407841463928768,3.93,79.97,58.3,11.85,19.63
cardiffnlp/twitter-xlm-roberta-base,277.0,250,512,14837,3.87,0.20686896516694914,0.17286001129010703,1.167298842197117,2.991735061708397,3.87,75.55,63.32,49.39,1.43
mistralai/Mistral-7B-v0.1 (few-shot),7242.0,32,32768,2657,3.85,0.9658862859101618,0.6030758914348258,2.094998859914024,0.9535800459975121,3.85,55.37,54.27,23.12,22.94
Twitter/twhin-bert-large,560.0,250,512,5299,3.84,0.2787877062055595,0.6030758914348258,1.8807789381629956,1.870786227352088,3.84,74.8,55.04,29.15,11.93
microsoft/xlm-align-base,277.0,250,512,14744,3.84,0.08263223993631871,0.3941036226306289,2.5619911431080125,1.6161168637770504,3.84,79.38,58.58,15.34,16.58
RuterNorway/Llama-2-13b-chat-norwegian (few-shot),unknown,32,4096,7778,3.79,0.9658862859101618,0.9741496987597942,2.2989370311300066,0.6003637728684872,3.79,56.71,49.77,19.92,27.87
jhu-clsp/bernice,277.0,250,128,5567,3.79,0.3758352936785263,0.17286001129010703,1.167298842197117,3.1303239863873533,3.79,72.25,62.0,48.1,0.0
mistralai/Mistral-7B-Instruct-v0.2 (few-shot),7242.0,32,32768,2538,3.75,0.9658862859101618,0.9741496987597942,2.094998859914024,0.9535800459975121,3.75,55.15,47.85,24.29,23.98
mistralai/Mistral-7B-Instruct-v0.1 (few-shot),7242.0,32,32768,5443,3.74,1.152647728426251,0.9741496987597942,2.2989370311300066,0.6003637728684872,3.74,51.79,47.27,22.15,24.3
clips/mfaq,277.0,250,128,5591,3.7,0.20686896516694914,0.3941036226306289,1.6006235902411823,2.991735061708397,3.7,76.68,59.51,32.54,1.53
01-ai/Yi-6B (few-shot),6061.0,64,4096,2786,3.66,1.4118297578534613,0.7691364013388334,2.7908047730441052,0.36851342415839494,3.66,44.97,53.14,7.64,30.12
sentence-transformers/paraphrase-xlm-r-multilingual-v1,277.0,250,512,14994,3.63,0.3758352936785263,0.3941036226306289,1.6006235902411823,3.102918960923735,3.63,71.8,57.71,38.75,0.3
meta-llama/Llama-2-7b-chat-hf (few-shot),6738.0,32,4096,2643,3.61,1.2207280433432017,1.1949687753430165,2.5619911431080125,0.6003637728684872,3.61,50.0,46.54,15.3,25.57
dbmdz/bert-base-historic-multilingual-cased,110.0,32,512,15165,3.58,0.5659130561605081,1.510463428571375,1.167298842197117,2.4270108430471358,3.58,67.24,41.36,48.21,7.75
sentence-transformers/stsb-xlm-r-multilingual,277.0,250,512,15040,3.54,0.48387232438022976,0.7691364013388334,1.6006235902411823,2.991735061708397,3.54,69.4,52.68,34.44,0.73
Rijgersberg/GEITje-7B (few-shot),7242.0,32,32768,10401,3.53,1.528857125778267,0.9741496987597942,2.7908047730441052,0.6003637728684872,3.53,39.09,47.83,10.31,26.13
sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2,118.0,250,512,17428,3.47,0.6277204589858295,0.6030758914348258,1.8807789381629956,2.991735061708397,3.47,65.62,55.6,32.22,0.64
AI-Sweden-Models/roberta-large-1160k,354.0,50,512,5741,3.45,0.48387232438022976,1.1949687753430165,3.0897646675429176,1.4206163223898995,3.45,68.39,45.91,2.79,18.83
AI-Sweden-Models/roberta-large-1350k,354.0,50,512,5744,3.43,0.5659130561605081,1.1949687753430165,3.0897646675429176,1.4206163223898995,3.43,67.24,45.84,2.28,18.17
meta-llama/Llama-2-7b-hf (few-shot),6738.0,32,4096,2648,3.38,1.528857125778267,0.9741496987597942,2.5619911431080125,1.4206163223898995,3.38,41.88,50.17,15.82,18.35
sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking,135.0,120,512,26151,3.27,0.5659130561605081,0.9741496987597942,2.2989370311300066,3.0775989641426267,3.27,66.75,49.61,17.6,0.36
sentence-transformers/distiluse-base-multilingual-cased-v1,135.0,120,512,26344,3.19,1.077480968538426,0.7691364013388334,2.2989370311300066,3.0775989641426267,3.19,53.77,53.53,20.15,0.58
sentence-transformers/distiluse-base-multilingual-cased-v2,135.0,120,512,17807,3.19,1.077480968538426,0.7691364013388334,2.2989370311300066,3.102918960923735,3.19,53.13,51.94,17.74,0.06
AI-Sweden-Models/gpt-sw3-20b (few-shot),20918.0,64,2048,4880,2.98,1.7606993526620003,1.8242388645168137,3.0897646675429176,1.4206163223898995,2.98,35.78,34.13,2.18,17.99
RuterNorway/Llama-2-7b-chat-norwegian (few-shot),unknown,32,4096,10890,2.92,2.085458542908788,1.8242388645168137,3.167399312353022,1.2407841463928768,2.92,27.22,33.54,0.45,20.4
Qwen/Qwen1.5-1.8B (few-shot),1837.0,152,32768,5666,2.65,3.1076922224616204,1.510463428571375,3.167399312353022,1.6161168637770504,2.65,0.03,38.3,0.39,16.67
Qwen/Qwen1.5-1.8B-Chat (few-shot),1837.0,152,32768,8304,2.6,3.0642885244945917,1.8242388645168137,3.0897646675429176,1.6161168637770504,2.6,1.45,36.21,3.12,16.36
3ebdola/Dialectal-Arabic-XLM-R-Base,277.0,250,512,15177,2.54,1.7606993526620003,1.8242388645168137,3.167399312353022,3.0775989641426267,2.54,34.18,34.81,1.25,0.46
Qwen/Qwen1.5-0.5B (few-shot),620.0,152,32768,11371,2.17,2.8969286157861522,3.382361721477208,3.167399312353022,1.870786227352088,2.17,5.85,10.64,0.33,11.81
Qwen/Qwen1.5-0.5B-Chat (few-shot),620.0,152,32768,11740,2.15,2.969307787873243,3.382361721477208,3.167399312353022,1.870786227352088,2.15,3.95,9.31,1.11,13.61
fresh-electra-small,13.0,31,512,7219,1.89,2.7614427862291118,3.382361721477208,3.167399312353022,3.1303239863873533,1.89,9.42,9.76,-0.18,0.0
RJuro/kanelsnegl-v0.1 (few-shot),7242.0,32,512,9757,1.65,3.1076922224616204,4.012571901431656,3.167399312353022,3.1303239863873533,1.65,0.0,0.0,0.0,0.0
ai-forever/mGPT (few-shot),unknown,100,1024,13551,1.65,3.1076922224616204,4.012571901431656,3.167399312353022,3.1303239863873533,1.65,0.3,0.29,-0.11,0.0
